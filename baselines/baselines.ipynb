{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-28T15:48:09.410469Z",
     "start_time": "2024-07-28T15:48:09.405483Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import linear_model\n",
    "from embeddings import *\n",
    "from models import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T15:48:09.690744Z",
     "start_time": "2024-07-28T15:48:09.686731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "RANDOM_SEED = 1\n",
    "DATA_FOLDER = \"twitter-datasets/\"\n",
    "TRAIN_NEG_FILE = DATA_FOLDER + \"train_neg.txt\"\n",
    "TRAIN_POS_FILE = DATA_FOLDER + \"train_pos.txt\""
   ],
   "id": "6117b3142096302f",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T15:48:09.950208Z",
     "start_time": "2024-07-28T15:48:09.942229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_dataset(neg_tweets_file, pos_tweets_file):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    with open(neg_tweets_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            tweets.append(line.rstrip())\n",
    "            labels.append(-1)\n",
    "    with open(pos_tweets_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            tweets.append(line.rstrip())\n",
    "            labels.append(1)\n",
    "    df = pd.DataFrame({\"tweet\": tweets, \"label\": labels})\n",
    "    df = df.drop_duplicates(subset=[\"tweet\", \"label\"])\n",
    "    return df\n",
    "\n",
    "def split_dataset(dataset, test_fraction, seed=RANDOM_SEED):\n",
    "    shuffled_dataset = dataset.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    train_df, test_df = train_test_split(shuffled_dataset, test_size=test_fraction, random_state=seed)\n",
    "    return train_df, test_df\n",
    "\n",
    "def get_data(embedding):\n",
    "    print(\"Preparing data...\")\n",
    "    dataset = create_dataset(TRAIN_NEG_FILE, TRAIN_POS_FILE)\n",
    "    train_data, test_data = split_dataset(dataset, 0.1, seed=RANDOM_SEED)\n",
    "    train_labels = train_data[\"label\"].to_numpy()\n",
    "    test_labels = test_data[\"label\"].to_numpy()\n",
    "\n",
    "    print(\"Getting embedding...\")\n",
    "    embedding.fit(train_data[\"tweet\"].to_numpy())\n",
    "    train_tweets = embedding.encode(train_data[\"tweet\"].to_numpy(), \"train_tweets\")\n",
    "    test_tweets = embedding.encode(test_data[\"tweet\"].to_numpy(), \"test_tweets\")\n",
    "\n",
    "    return train_tweets, train_labels, test_tweets, test_labels"
   ],
   "id": "a12ea63179a6a57c",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T15:48:10.211017Z",
     "start_time": "2024-07-28T15:48:10.206031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_accuracy(predicted, actual):\n",
    "    assert predicted.shape == actual.shape\n",
    "    return (predicted == actual).mean()"
   ],
   "id": "6285d0564fef9446",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T15:48:10.456479Z",
     "start_time": "2024-07-28T15:48:10.448106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def make_learning_curves(model, train_tweets, train_labels, test_tweets, test_labels, title):\n",
    "    training_size_max = train_tweets.shape[0]\n",
    "    num_steps = 5\n",
    "    step_size = int(train_tweets.shape[0] / num_steps)\n",
    "    training_size_min = step_size\n",
    "    dataset_sizes = np.arange(training_size_min, training_size_max, step_size)\n",
    "\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    for train_size in tqdm(dataset_sizes, desc=f\"Making learning curves...\"):\n",
    "        model.train(train_tweets[:train_size], train_labels[:train_size])\n",
    "        train_pred = model.predict(train_tweets)\n",
    "        test_pred = model.predict(test_tweets)\n",
    "        train_accuracy.append(evaluate_accuracy(train_pred, train_labels))\n",
    "        test_accuracy.append(evaluate_accuracy(test_pred, test_labels))\n",
    "\n",
    "    plot_learning_curves(train_accuracy, test_accuracy, dataset_sizes, title)\n",
    "\n",
    "\n",
    "def plot_learning_curves(train_accuracy, test_accuracy, dataset_sizes, title):\n",
    "    plt.figure()\n",
    "    plt.plot(dataset_sizes, train_accuracy, 'r--', label=\"Training Accuracy\")\n",
    "    plt.plot(dataset_sizes, test_accuracy, 'b-', label=\"Test Accuracy\")\n",
    "    plt.ylim([0.4, 1])\n",
    "    plt.yticks(np.arange(0.4, 1.1, 0.1))\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"Dataset Size\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "2e5a878608e11ca4",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T15:48:10.697833Z",
     "start_time": "2024-07-28T15:48:10.693842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_baseline(embedding, model):\n",
    "    train_tweets, train_labels, test_tweets, test_labels = get_data(embedding)\n",
    "\n",
    "    print(f\"Training {model} with {embedding}...\")\n",
    "    make_learning_curves(model, train_tweets, train_labels, test_tweets, test_labels, title=f\"{model} with {embedding}\")\n",
    "    return evaluate_accuracy(model.predict(test_tweets), test_labels)"
   ],
   "id": "718a381039eab935",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T15:48:11.035640Z",
     "start_time": "2024-07-28T15:48:10.938212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Bag of Words baseline:\")\n",
    "embedding = BagOfWords()\n",
    "model = LogisticRegression()\n",
    "final_accuracy = evaluate_baseline(embedding, model)\n",
    "print(f\"Final accuracy: {final_accuracy}\")\n",
    "\n",
    "print(\"Roberta baseline:\")\n",
    "embedding = RobertaBaseSentimentEmbedding(load_embeddings=True)\n",
    "model = LogisticRegression()\n",
    "final_accuracy = evaluate_baseline(embedding, model)\n",
    "print(f\"Final accuracy: {final_accuracy}\")"
   ],
   "id": "2b232c5c34d2a4da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words baseline:\n",
      "Preparing data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '././twitter-datasets/train_neg.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[42], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m embedding \u001B[38;5;241m=\u001B[39m BagOfWords()\n\u001B[0;32m      5\u001B[0m model \u001B[38;5;241m=\u001B[39m LogisticRegression()\n\u001B[1;32m----> 6\u001B[0m final_accuracy \u001B[38;5;241m=\u001B[39m evaluate_baseline(embedding, model)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFinal accuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfinal_accuracy\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRoberta baseline:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[41], line 2\u001B[0m, in \u001B[0;36mevaluate_baseline\u001B[1;34m(embedding, model)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate_baseline\u001B[39m(embedding, model):\n\u001B[1;32m----> 2\u001B[0m     train_tweets, train_labels, test_tweets, test_labels \u001B[38;5;241m=\u001B[39m get_data(embedding)\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00membedding\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m     make_learning_curves(model, train_tweets, train_labels, test_tweets, test_labels, title\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00membedding\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[38], line 23\u001B[0m, in \u001B[0;36mget_data\u001B[1;34m(embedding)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_data\u001B[39m(embedding):\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPreparing data...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 23\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m create_dataset(TRAIN_NEG_FILE, TRAIN_POS_FILE)\n\u001B[0;32m     24\u001B[0m     train_data, test_data \u001B[38;5;241m=\u001B[39m split_dataset(dataset, \u001B[38;5;241m0.1\u001B[39m, seed\u001B[38;5;241m=\u001B[39mRANDOM_SEED)\n\u001B[0;32m     25\u001B[0m     train_labels \u001B[38;5;241m=\u001B[39m train_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mto_numpy()\n",
      "Cell \u001B[1;32mIn[38], line 4\u001B[0m, in \u001B[0;36mcreate_dataset\u001B[1;34m(neg_tweets_file, pos_tweets_file)\u001B[0m\n\u001B[0;32m      2\u001B[0m tweets \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      3\u001B[0m labels \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(neg_tweets_file, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m f:\n\u001B[0;32m      6\u001B[0m         tweets\u001B[38;5;241m.\u001B[39mappend(line\u001B[38;5;241m.\u001B[39mrstrip())\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[1;34m(file, *args, **kwargs)\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[0;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    308\u001B[0m     )\n\u001B[1;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m io_open(file, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '././twitter-datasets/train_neg.txt'"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T15:48:11.261230Z",
     "start_time": "2024-07-28T15:48:11.257241Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e7bb300eb992c325",
   "outputs": [],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
