{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-09T10:34:10.959389Z",
     "start_time": "2024-07-09T10:34:10.954453Z"
    }
   },
   "source": [
    "import pickle\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import islice\n",
    "from typing import Literal, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import wandb\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from torch import Tensor\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import AutoTokenizer, AutoModel"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T10:34:10.966544Z",
     "start_time": "2024-07-09T10:34:10.962351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, embedding_model, head_model):\n",
    "        super().__init__()\n",
    "        self.embedding_model = embedding_model\n",
    "        self.head_model = head_model\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding = self.embedding_model(**x).last_hidden_state[:, 0, :]\n",
    "        output = self.head_model(embedding)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ],
   "id": "60f493968cec5a06",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T10:34:10.988205Z",
     "start_time": "2024-07-09T10:34:10.978419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_step(model, dataloader):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        x, y_true = batch['tokens'], batch['labels']\n",
    "        x = x.to(Config.device)\n",
    "        y_true = y_true.to(Config.device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        del batch\n",
    "        del x\n",
    "        del y_true\n",
    "        del y_pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    mean_loss = np.array(losses).mean()\n",
    "    if Config.use_wandb:\n",
    "        wandb.log({\"train loss\": mean_loss})\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def valid_step(model, dataloader, should_be_neg, should_be_pos):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    scores = []\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        x, y_true = batch['tokens'], batch['labels']\n",
    "        x = x.to(Config.device)\n",
    "        y_true = y_true.to(Config.device)\n",
    "        y_pred = model(x)\n",
    "\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        y_pred = y_pred.cpu().numpy()\n",
    "        y_pred[y_pred >= .5] = 1\n",
    "        y_pred[y_pred < .5] = 0\n",
    "        score = (y_pred == y_true)\n",
    "        scores.append(score)\n",
    "\n",
    "        del batch\n",
    "        del x\n",
    "        del y_true\n",
    "        del y_pred\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def append_to_scores(df: pd.DataFrame, expected_label: int):\n",
    "        arr = np.array(list([b] for b in df['label'] == expected_label))\n",
    "        if len(arr):\n",
    "            scores.append(arr)\n",
    "            \n",
    "    append_to_scores(should_be_neg, 0)\n",
    "    append_to_scores(should_be_pos, 1)\n",
    "\n",
    "    scores = np.vstack(scores)\n",
    "    accuracy = np.array(scores).mean()\n",
    "    mean_loss = np.array(losses).mean()\n",
    "    if Config.use_wandb:\n",
    "        wandb.log({\"valid loss\": mean_loss})\n",
    "        wandb.log({\"valid accuracy\": accuracy})\n",
    "    return mean_loss, accuracy"
   ],
   "id": "b46da006a40a4ba",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T10:34:11.631975Z",
     "start_time": "2024-07-09T10:34:11.023116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Name\n",
    "    comment: str = \"cool name for my charts\"\n",
    "    \n",
    "    # Stats\n",
    "    use_wandb: bool = False\n",
    "    \n",
    "    # Saving model and info\n",
    "    records_filename: str = \"records.csv\"\n",
    "    model_filename_format: str = \"models/model{:02}.pkl\"\n",
    "    config_filename_format: str = \"models/config{:02}.pkl\"\n",
    "\n",
    "    # Data\n",
    "    no_of_samples: int = 200000\n",
    "    validation_size: float = 0.05\n",
    "    train_file_neg: str = 'twitter-datasets/train_neg.txt'\n",
    "    train_file_pos: str = 'twitter-datasets/train_pos.txt'\n",
    "    test_file: str = 'twitter-datasets/test_data.txt'\n",
    "    \n",
    "    # Pre-trained model\n",
    "    model_name: str = 'albert-base-v2'\n",
    "    \n",
    "    # Hyperparameters\n",
    "    epochs: int = 10\n",
    "    batch_size: int = 20\n",
    "    learning_rate: float = 1e-4\n",
    "    weight_decay = float = 1e-4\n",
    "    scheduler_step_size: int = 5\n",
    "    scheduler_gamma: float = 0.5\n",
    "\n",
    "    # LoRA\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_target_modules: [str] = field(default_factory=lambda: [\"query\", \"value\"])\n",
    "    lora_dropout: float = 0.4\n",
    "    lora_bias: Literal[\"none\", \"all\", \"lora_only\"] = \"none\"\n",
    "\n",
    "    # Head model\n",
    "    head_model_str: str = field(default_factory=lambda: str(head_model))\n",
    "    last_layer_size = 64\n",
    "    \n",
    "    # Other stuff\n",
    "    device: str = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    random_seed: int = 666\n",
    "\n",
    "\n",
    "embedding_model = AutoModel.from_pretrained(Config.model_name, resume_download=None)\n",
    "head_model = nn.Sequential(nn.Linear(embedding_model.config.hidden_size, Config.last_layer_size),\n",
    "                           nn.ReLU(),\n",
    "                           nn.Linear(Config.last_layer_size, 1))\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=Config.lora_r,\n",
    "    lora_alpha=Config.lora_alpha,\n",
    "    target_modules=Config().lora_target_modules,\n",
    "    lora_dropout=Config.lora_dropout,\n",
    "    bias=Config.lora_bias,\n",
    ")\n",
    "lora_model = get_peft_model(embedding_model, lora_config)\n",
    "my_model = MyModel(lora_model, head_model).to(Config.device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = AdamW(my_model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=Config.scheduler_step_size, gamma=Config.scheduler_gamma)"
   ],
   "id": "5504cc9c67dd9b7b",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if Config.use_wandb:\n",
    "    wandb.init(\n",
    "        project=\"CIL Project\",\n",
    "        name=Config().comment,\n",
    "        config=Config().__dict__\n",
    "    )\n",
    "    wandb.watch(my_model, log_freq=100)"
   ],
   "id": "bd00e2b7e8a9f456",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T10:34:23.230308Z",
     "start_time": "2024-07-09T10:34:23.221586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_train_data(input_parsing_funs: [Callable[[str], bool]] = None) -> pd.DataFrame:\n",
    "    tweets, labels = [], []\n",
    "\n",
    "    def load_tweets(filename, label):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            count = Config.no_of_samples // 2\n",
    "            for line in tqdm.tqdm(islice(f, count), total=count, desc='Loading Tweets'):\n",
    "                line = line.rstrip()\n",
    "                for fun in input_parsing_funs:\n",
    "                    line = fun(line)\n",
    "                if line not in tweets:\n",
    "                    tweets.append(line)\n",
    "                    labels.append(label)\n",
    "\n",
    "    load_tweets(Config.train_file_neg, 0)\n",
    "    load_tweets(Config.train_file_pos, 1)\n",
    "\n",
    "    return pd.DataFrame(data={'tweet': tweets, 'label': labels})\n",
    "\n",
    "\n",
    "class InputParsing:\n",
    "    @staticmethod\n",
    "    def remove_users(words):\n",
    "        return ' '.join([word for word in words.split() if not word == '<user>'])\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_hashtags(words):\n",
    "        return ' '.join([word for word in words.split() if not word.startswith('#')])\n",
    "\n",
    "    @staticmethod\n",
    "    def unify_hashtags(words):\n",
    "        f = lambda word: '<hashtag>' if word.startswith('#') else word\n",
    "        return ' '.join([f(word) for word in words.split()])\n",
    "\n",
    "    @staticmethod\n",
    "    def unify_numbers(words):\n",
    "        f = lambda word: '<number>' if word.isnumeric() else word\n",
    "        return ' '.join([f(word) for word in words.split()])"
   ],
   "id": "75924da821612005",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T10:34:23.236190Z",
     "start_time": "2024-07-09T10:34:23.231293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataframe.iloc[index]"
   ],
   "id": "a71a70e0e17c9eed",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T10:34:23.245091Z",
     "start_time": "2024-07-09T10:34:23.238162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_dataset(dataset):\n",
    "    valid_size = int(Config.validation_size * len(dataset))\n",
    "    train_size = len(dataset) - valid_size\n",
    "    generator = torch.Generator().manual_seed(Config.random_seed)\n",
    "    train_split, valid_split = random_split(dataset, [train_size, valid_size], generator=generator)\n",
    "    return dataset.dataframe.iloc[train_split.indices], dataset.dataframe.iloc[valid_split.indices]\n",
    "\n",
    "\n",
    "def get_dataframes(input_parsing_funs: [Callable[[str], bool]] = None):\n",
    "    return split_dataset(TweetDataset(load_train_data(input_parsing_funs)))"
   ],
   "id": "6c1e3b0691b8da85",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T10:34:23.253339Z",
     "start_time": "2024-07-09T10:34:23.246081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_by_filter(\n",
    "        df: pd.DataFrame, \n",
    "        filter_fun: Callable[[str], bool]) -> (pd.DataFrame, pd.DataFrame):\n",
    "    filtered = df['tweet'].apply(filter_fun)\n",
    "    return df[filtered == True], df[filtered == False]\n",
    "\n",
    "\n",
    "def split_into_neg_unknown_pos(\n",
    "        df: pd.DataFrame, \n",
    "        neg_filter: Callable[[str], bool],\n",
    "        pos_filter: Callable[[str], bool]) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    pos, rest = split_by_filter(df, pos_filter)\n",
    "    neg, unknown = split_by_filter(rest, neg_filter)\n",
    "    return neg, unknown, pos\n",
    "\n",
    "\n",
    "class TweetFilters:\n",
    "    @staticmethod\n",
    "    def unclosed_parenthesis(tweet):\n",
    "        return tweet.count('(') > tweet.count(')')\n",
    "\n",
    "    @staticmethod\n",
    "    # EXAMPLE FILTER FUNCTION\n",
    "    def has_word_frame(tweet):\n",
    "        return 'frame' in tweet\n",
    "\n",
    "    @staticmethod\n",
    "    # EXAMPLE FILTER FUNCTION\n",
    "    def has_word_thanks(tweet):\n",
    "        return 'thanks' in tweet\n",
    "    \n",
    "    @staticmethod\n",
    "    def no_filter(tweet):\n",
    "        return False"
   ],
   "id": "417ac8a183363815",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_parsing_funs = [\n",
    "    InputParsing.unify_hashtags,\n",
    "]\n",
    "neg_filter = TweetFilters.has_word_frame\n",
    "pos_filter = TweetFilters.has_word_thanks\n",
    "\n",
    "train_df, valid_df = get_dataframes(input_parsing_funs)\n",
    "_, train_df, _ = split_into_neg_unknown_pos(train_df, neg_filter, pos_filter)\n",
    "neg_valid, valid_df, pos_valid = split_into_neg_unknown_pos(valid_df, neg_filter, pos_filter)\n",
    "\n",
    "train_dataset = TweetDataset(train_df)\n",
    "valid_dataset = TweetDataset(valid_df)"
   ],
   "id": "91fec64fa0288abf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T10:42:47.615509Z",
     "start_time": "2024-07-09T10:42:47.609081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gen_tokenize_fun():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.model_name, resume_download=None)\n",
    "\n",
    "    def tokenize(data):\n",
    "        tweets = [x[\"tweet\"] for x in data]\n",
    "        labels = Tensor([[x[\"label\"]] for x in data])\n",
    "        output = tokenizer(tweets, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        return {\"tokens\": output, \"labels\": labels}\n",
    "\n",
    "    return tokenize\n",
    "\n",
    "\n",
    "def make_dataloader(dataset, shuffle: bool):\n",
    "    return DataLoader(dataset=dataset,\n",
    "                      collate_fn=gen_tokenize_fun(),\n",
    "                      batch_size=Config.batch_size,\n",
    "                      shuffle=shuffle,\n",
    "                      pin_memory=True)\n",
    "\n",
    "\n",
    "def get_dataloaders(train_dataset, valid_dataset):\n",
    "    return (make_dataloader(train_dataset, shuffle=True),\n",
    "            make_dataloader(valid_dataset, shuffle=False))"
   ],
   "id": "530b0194d231c9eb",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_loader, valid_loader = get_dataloaders(train_dataset, valid_dataset)\n",
    "\n",
    "for epoch in range(Config.epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{Config.epochs}:\")\n",
    "\n",
    "    train_loss = train_step(my_model, train_loader)\n",
    "    valid_loss, valid_accuracy = valid_step(my_model, valid_loader, neg_valid, pos_valid)\n",
    "\n",
    "    print(f\"  TRAIN loss     = {train_loss}\")\n",
    "    print(f\"  VALID loss     = {valid_loss}\")\n",
    "    print(f\"  VALID accuracy = {valid_accuracy}\")\n",
    "    print(f\"--------------------------------------------------------\")\n",
    "\n",
    "    scheduler.step()"
   ],
   "id": "d48e124c3eda5303",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 56,
   "source": [
    "def save_model(model, accuracy: float, description: str = ''):\n",
    "    records = pd.read_csv(Config.records_filename, index_col='Index')\n",
    "    index = len(records)\n",
    "    model_filename = Config.model_filename_format.format(index)\n",
    "    config_filename = Config.config_filename_format.format(index)\n",
    "\n",
    "    records.loc[model_filename] = {'Accuracy': accuracy, 'Description': description}\n",
    "    pickle.dump(model, open(model_filename, 'wb'))\n",
    "    pickle.dump(Config(), open(config_filename, 'wb'))\n",
    "    records.to_csv(Config.records_filename)\n",
    "\n",
    "\n",
    "def load_model(index):\n",
    "    model_filename = Config.model_filename_format.format(index)\n",
    "    return pickle.load(open(model_filename, 'rb'))\n",
    "\n",
    "\n",
    "def load_config(index):\n",
    "    config_filename = Config.config_filename_format.format(index)\n",
    "    return pickle.load(open(config_filename, 'rb'))\n",
    "\n",
    "\n",
    "def load_records():\n",
    "    return pd.read_csv(Config.records_filename, index_col='Index')"
   ],
   "id": "4cabf95118c0c297"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T12:46:18.229544Z",
     "start_time": "2024-07-09T12:46:18.082351Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 66,
   "source": "save_model(my_model, valid_accuracy, Config.comment)",
   "id": "11e2f71074b0004c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "load_records()",
   "id": "4a38a8f32762b91c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if Config.use_wandb:\n",
    "    wandb.finish()"
   ],
   "id": "ba76838e75aa02ca",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
